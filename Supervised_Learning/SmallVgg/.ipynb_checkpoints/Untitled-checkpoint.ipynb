{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial Using Dog, Cat, and Panda Images (based on pyimagesearch.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all dependencies \n",
    "import matplotlib \n",
    "matplotlib.use(\"Agg\") #using Agg backend to enble saving plots to disk \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense \n",
    "from keras.optimizers import SGD \n",
    "from keras import backend as bk\n",
    "from imutils import paths \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import argparse \n",
    "import random \n",
    "import pickle \n",
    "import cv2 as cv\n",
    "import os \n",
    "\n",
    "\n",
    "#checking to see if it is using gpu backend \n",
    "bk.tensorflow_backend._get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -d DATASET -m MODEL -l LABEL_BIN -p PLOT\n",
      "ipykernel_launcher.py: error: the following arguments are required: -d/--dataset, -m/--model, -l/--label-bin, -p/--plot\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaddy/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", required=True, help=\"path to input dataset of images\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True, help=\"path to output trained model\")\n",
    "ap.add_argument(\"-l\", \"--label-bin\", required=True, help=\"path to output label binarizer\")\n",
    "ap.add_argument(\"-p\", \"--plot\", required=True, help=\"path to output accuracy/loss plot\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Resizing the Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of labels 3000\n",
      "length of data 3072\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "pixel_new = 32\n",
    "\n",
    "dataPath = '/home/chaddy/Keras_Testing/keras-tutorial/animals'\n",
    "\n",
    "imagePaths = sorted(list(paths.list_images(dataPath)))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    image = cv.imread(imagePath)\n",
    "    #resize the images into 32x32 pixel image with the aspect ratio ignored. \n",
    "    #Then each image is flattened into 332x32x3 pixel image \n",
    "    image = cv.resize(image, (pixel_new, pixel_new)).flatten()\n",
    "    data.append(image)\n",
    "    \n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "    \n",
    "print(\"length of labels\", len(labels)) #should be 3000: 1000 along 3 animal types\n",
    "print(\"length of data\", len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3072) (3000,)\n"
     ]
    }
   ],
   "source": [
    "data  = np.array(data, dtype=\"float\")/255.0 #normalizing the pixel data to go from [0,1] instead of [0,255]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (2025, 3072) val size (225, 3072) test size (750, 3072)\n"
     ]
    }
   ],
   "source": [
    "#use 75% of the data for training and the remiaingin 25% for testing using scikit learn \n",
    "(trainX_with_val, testX, trainY_with_val, testY) = train_test_split(data, labels, test_size = 0.25, random_state = 42)\n",
    "(trainX, valX, trainY, valY) = train_test_split(trainX_with_val, trainY_with_val, test_size = 0.1, random_state = 60)\n",
    "print(\"train size\", trainX.shape, \"val size\", valX.shape, \"test size\", testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['cats' 'panda' 'cats' ... 'dogs' 'dogs' 'dogs']\n",
      "After [[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "3\n",
      "['cats' 'dogs' 'panda']\n"
     ]
    }
   ],
   "source": [
    "#convert the labels from integers to vectors for one hot encoding \n",
    "\n",
    "lb = LabelBinarizer()\n",
    "print(\"Before\", trainY) #the labels are still in string form \n",
    "lb.fit(trainY) #find the unique class labels in the data\n",
    "trainY = lb.transform(trainY) #transforms the labels to one hot encoding using the  information from fit \n",
    "print(\"After\", trainY) #one hot encoding vector \n",
    "valY = lb.transform(valY)\n",
    "testY = lb.transform(testY)#same thing done for test data\n",
    "n_classes = len(lb.classes_)\n",
    "print(n_classes)\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one input layer, two hidden layers, and one output layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "inputDim = pixel_new*pixel_new*3\n",
    "#the data is vector with size 3072 (32x32x3)\n",
    "model.add(Dense(1024, input_dim=inputDim, activation = \"sigmoid\")) #only need to specify the input dimension to the first layers \n",
    "model.add(Dense(512, activation = \"sigmoid\"))\n",
    "model.add(Dense(n_classes, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning the Created Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.01\n",
    "EPOCHS = 75\n",
    "\n",
    "opt = SGD(lr=INIT_LR) #define stochastic gradient dsecent as the optimizer with the initial learning rate\n",
    "#using categorial crossentropy as the loss fucntion. Use accuracy as metrics\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Network...\n",
      "Train on 2025 samples, validate on 225 samples\n",
      "Epoch 1/75\n",
      "2025/2025 [==============================] - 0s 67us/step - loss: 0.7751 - acc: 0.6128 - val_loss: 0.8635 - val_acc: 0.5378\n",
      "Epoch 2/75\n",
      "2025/2025 [==============================] - 0s 56us/step - loss: 0.7751 - acc: 0.6084 - val_loss: 0.9177 - val_acc: 0.5111\n",
      "Epoch 3/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7723 - acc: 0.6119 - val_loss: 0.8395 - val_acc: 0.5778\n",
      "Epoch 4/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7725 - acc: 0.6054 - val_loss: 0.8245 - val_acc: 0.6089\n",
      "Epoch 5/75\n",
      "2025/2025 [==============================] - 0s 62us/step - loss: 0.7721 - acc: 0.6114 - val_loss: 0.8490 - val_acc: 0.5911\n",
      "Epoch 6/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7681 - acc: 0.6193 - val_loss: 0.8401 - val_acc: 0.5733\n",
      "Epoch 7/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7696 - acc: 0.6054 - val_loss: 0.8215 - val_acc: 0.6044\n",
      "Epoch 8/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7722 - acc: 0.6207 - val_loss: 0.8484 - val_acc: 0.5689\n",
      "Epoch 9/75\n",
      "2025/2025 [==============================] - 0s 61us/step - loss: 0.7685 - acc: 0.6158 - val_loss: 0.8627 - val_acc: 0.5467\n",
      "Epoch 10/75\n",
      "2025/2025 [==============================] - 0s 61us/step - loss: 0.7632 - acc: 0.6188 - val_loss: 0.8274 - val_acc: 0.6133\n",
      "Epoch 11/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7607 - acc: 0.6272 - val_loss: 0.8225 - val_acc: 0.6444\n",
      "Epoch 12/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7636 - acc: 0.6173 - val_loss: 0.8334 - val_acc: 0.5956\n",
      "Epoch 13/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7592 - acc: 0.6237 - val_loss: 0.8497 - val_acc: 0.5467\n",
      "Epoch 14/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7647 - acc: 0.6153 - val_loss: 0.8606 - val_acc: 0.5556\n",
      "Epoch 15/75\n",
      "2025/2025 [==============================] - 0s 62us/step - loss: 0.7591 - acc: 0.6158 - val_loss: 0.8403 - val_acc: 0.6089\n",
      "Epoch 16/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7582 - acc: 0.6212 - val_loss: 0.8405 - val_acc: 0.5911\n",
      "Epoch 17/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.7559 - acc: 0.6301 - val_loss: 0.8191 - val_acc: 0.6533\n",
      "Epoch 18/75\n",
      "2025/2025 [==============================] - 0s 61us/step - loss: 0.7519 - acc: 0.6331 - val_loss: 0.8354 - val_acc: 0.6000\n",
      "Epoch 19/75\n",
      "2025/2025 [==============================] - 0s 72us/step - loss: 0.7557 - acc: 0.6247 - val_loss: 0.8188 - val_acc: 0.6356\n",
      "Epoch 20/75\n",
      "2025/2025 [==============================] - 0s 65us/step - loss: 0.7505 - acc: 0.6262 - val_loss: 0.9642 - val_acc: 0.5644\n",
      "Epoch 21/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7527 - acc: 0.6301 - val_loss: 0.8794 - val_acc: 0.5600\n",
      "Epoch 22/75\n",
      "2025/2025 [==============================] - 0s 62us/step - loss: 0.7505 - acc: 0.6336 - val_loss: 0.8127 - val_acc: 0.6711\n",
      "Epoch 23/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7457 - acc: 0.6331 - val_loss: 0.8207 - val_acc: 0.6533\n",
      "Epoch 24/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7478 - acc: 0.6281 - val_loss: 0.8796 - val_acc: 0.5333\n",
      "Epoch 25/75\n",
      "2025/2025 [==============================] - 0s 61us/step - loss: 0.7517 - acc: 0.6281 - val_loss: 0.8249 - val_acc: 0.6044\n",
      "Epoch 26/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.7465 - acc: 0.6183 - val_loss: 0.8464 - val_acc: 0.6311\n",
      "Epoch 27/75\n",
      "2025/2025 [==============================] - 0s 62us/step - loss: 0.7422 - acc: 0.6395 - val_loss: 0.9060 - val_acc: 0.5378\n",
      "Epoch 28/75\n",
      "2025/2025 [==============================] - 0s 59us/step - loss: 0.7435 - acc: 0.6444 - val_loss: 0.8241 - val_acc: 0.6222\n",
      "Epoch 29/75\n",
      "2025/2025 [==============================] - 0s 59us/step - loss: 0.7440 - acc: 0.6252 - val_loss: 0.8792 - val_acc: 0.5822\n",
      "Epoch 30/75\n",
      "2025/2025 [==============================] - 0s 59us/step - loss: 0.7411 - acc: 0.6331 - val_loss: 0.9023 - val_acc: 0.5289\n",
      "Epoch 31/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.7416 - acc: 0.6252 - val_loss: 0.8431 - val_acc: 0.5733\n",
      "Epoch 32/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7334 - acc: 0.6489 - val_loss: 0.8172 - val_acc: 0.6356\n",
      "Epoch 33/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7359 - acc: 0.6474 - val_loss: 0.8862 - val_acc: 0.5467\n",
      "Epoch 34/75\n",
      "2025/2025 [==============================] - 0s 64us/step - loss: 0.7371 - acc: 0.6336 - val_loss: 0.8248 - val_acc: 0.6133\n",
      "Epoch 35/75\n",
      "2025/2025 [==============================] - ETA: 0s - loss: 0.7386 - acc: 0.640 - 0s 60us/step - loss: 0.7365 - acc: 0.6405 - val_loss: 0.8479 - val_acc: 0.6044\n",
      "Epoch 36/75\n",
      "2025/2025 [==============================] - 0s 56us/step - loss: 0.7321 - acc: 0.6385 - val_loss: 0.8485 - val_acc: 0.5911\n",
      "Epoch 37/75\n",
      "2025/2025 [==============================] - 0s 55us/step - loss: 0.7406 - acc: 0.6360 - val_loss: 0.8586 - val_acc: 0.5778\n",
      "Epoch 38/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7341 - acc: 0.6420 - val_loss: 0.8691 - val_acc: 0.5556\n",
      "Epoch 39/75\n",
      "2025/2025 [==============================] - 0s 59us/step - loss: 0.7332 - acc: 0.6449 - val_loss: 0.8330 - val_acc: 0.6400\n",
      "Epoch 40/75\n",
      "2025/2025 [==============================] - 0s 61us/step - loss: 0.7314 - acc: 0.6449 - val_loss: 0.8289 - val_acc: 0.6133\n",
      "Epoch 41/75\n",
      "2025/2025 [==============================] - 0s 69us/step - loss: 0.7277 - acc: 0.6528 - val_loss: 0.8344 - val_acc: 0.5778\n",
      "Epoch 42/75\n",
      "2025/2025 [==============================] - 0s 63us/step - loss: 0.7265 - acc: 0.6380 - val_loss: 0.8249 - val_acc: 0.6444\n",
      "Epoch 43/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7309 - acc: 0.6578 - val_loss: 0.8863 - val_acc: 0.5600\n",
      "Epoch 44/75\n",
      "2025/2025 [==============================] - 0s 69us/step - loss: 0.7246 - acc: 0.6464 - val_loss: 0.9041 - val_acc: 0.5244\n",
      "Epoch 45/75\n",
      "2025/2025 [==============================] - 0s 68us/step - loss: 0.7225 - acc: 0.6425 - val_loss: 0.9295 - val_acc: 0.5200\n",
      "Epoch 46/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7223 - acc: 0.6612 - val_loss: 0.8476 - val_acc: 0.5956\n",
      "Epoch 47/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.7199 - acc: 0.6519 - val_loss: 0.8170 - val_acc: 0.6489\n",
      "Epoch 48/75\n",
      "2025/2025 [==============================] - 0s 56us/step - loss: 0.7221 - acc: 0.6528 - val_loss: 0.9122 - val_acc: 0.5244\n",
      "Epoch 49/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7234 - acc: 0.6459 - val_loss: 0.8733 - val_acc: 0.5644\n",
      "Epoch 50/75\n",
      "2025/2025 [==============================] - 0s 59us/step - loss: 0.7159 - acc: 0.6583 - val_loss: 0.8608 - val_acc: 0.5911\n",
      "Epoch 51/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.7172 - acc: 0.6563 - val_loss: 0.8271 - val_acc: 0.6311\n",
      "Epoch 52/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7144 - acc: 0.6553 - val_loss: 0.8567 - val_acc: 0.5733\n",
      "Epoch 53/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.7105 - acc: 0.6543 - val_loss: 0.8978 - val_acc: 0.5867\n",
      "Epoch 54/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7155 - acc: 0.6449 - val_loss: 0.8676 - val_acc: 0.5956\n",
      "Epoch 55/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.7073 - acc: 0.6612 - val_loss: 0.8336 - val_acc: 0.5911\n",
      "Epoch 56/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7031 - acc: 0.6657 - val_loss: 0.8889 - val_acc: 0.5378\n",
      "Epoch 57/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7103 - acc: 0.6430 - val_loss: 0.8500 - val_acc: 0.6267\n",
      "Epoch 58/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7098 - acc: 0.6435 - val_loss: 0.9013 - val_acc: 0.5600\n",
      "Epoch 59/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7037 - acc: 0.6681 - val_loss: 0.8547 - val_acc: 0.6311\n",
      "Epoch 60/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 0s 56us/step - loss: 0.7006 - acc: 0.6637 - val_loss: 0.8623 - val_acc: 0.6044\n",
      "Epoch 61/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7008 - acc: 0.6691 - val_loss: 0.9154 - val_acc: 0.5200\n",
      "Epoch 62/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.7027 - acc: 0.6573 - val_loss: 0.9327 - val_acc: 0.5156\n",
      "Epoch 63/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.6964 - acc: 0.6647 - val_loss: 0.8417 - val_acc: 0.6044\n",
      "Epoch 64/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.6964 - acc: 0.6637 - val_loss: 0.8245 - val_acc: 0.6311\n",
      "Epoch 65/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.6975 - acc: 0.6657 - val_loss: 1.0313 - val_acc: 0.4933\n",
      "Epoch 66/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.6987 - acc: 0.6696 - val_loss: 0.9515 - val_acc: 0.5333\n",
      "Epoch 67/75\n",
      "2025/2025 [==============================] - 0s 56us/step - loss: 0.6950 - acc: 0.6588 - val_loss: 0.8451 - val_acc: 0.5644\n",
      "Epoch 68/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.6930 - acc: 0.6686 - val_loss: 0.9750 - val_acc: 0.5022\n",
      "Epoch 69/75\n",
      "2025/2025 [==============================] - 0s 58us/step - loss: 0.6906 - acc: 0.6780 - val_loss: 0.8314 - val_acc: 0.6000\n",
      "Epoch 70/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.6861 - acc: 0.6736 - val_loss: 0.8822 - val_acc: 0.5422\n",
      "Epoch 71/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.6936 - acc: 0.6632 - val_loss: 0.8517 - val_acc: 0.5867\n",
      "Epoch 72/75\n",
      "2025/2025 [==============================] - 0s 63us/step - loss: 0.6883 - acc: 0.6726 - val_loss: 0.9176 - val_acc: 0.6133\n",
      "Epoch 73/75\n",
      "2025/2025 [==============================] - 0s 63us/step - loss: 0.6817 - acc: 0.6825 - val_loss: 0.8254 - val_acc: 0.6267\n",
      "Epoch 74/75\n",
      "2025/2025 [==============================] - 0s 57us/step - loss: 0.6790 - acc: 0.6864 - val_loss: 0.8306 - val_acc: 0.6489\n",
      "Epoch 75/75\n",
      "2025/2025 [==============================] - 0s 60us/step - loss: 0.6851 - acc: 0.6775 - val_loss: 0.8397 - val_acc: 0.6400\n"
     ]
    }
   ],
   "source": [
    "#train the newural network \n",
    "\n",
    "print(\"Traning Network...\")\n",
    "#Epoch:When the entire dataset  is passed forward and backward through the nerural network once\n",
    "#batch size:  Total number of training examples present in a single batch(dividing the dataset into batches)\n",
    "Fit = model.fit(trainX, trainY, validation_data= (valX, valY), epochs=EPOCHS, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction with the Trained Network  using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.62      0.30      0.41       236\n",
      "        dogs       0.45      0.58      0.51       236\n",
      "       panda       0.68      0.83      0.75       278\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       750\n",
      "   macro avg       0.59      0.57      0.55       750\n",
      "weighted avg       0.59      0.58      0.57       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1), target_names=lb.classes_))\n",
    "\n",
    "#Plot the training loss and accuracy\n",
    "N = np.arange(0,EPOCHS)\n",
    "plt.figure()\n",
    "plt.plot(N, Fit.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, Fit.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, Fit.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, Fit.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (Simple NN)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/home/chaddy/Keras_Testing/result\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction on New Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3072)\n",
      "[[0.00204683 0.00551337 0.99243975]]\n"
     ]
    }
   ],
   "source": [
    "catPath = '/home/chaddy/Keras_Testing/keras-tutorial/images/cat.jpg'\n",
    "dogPath = '/home/chaddy/Keras_Testing/keras-tutorial/images/dog.jpg'\n",
    "pandaPath = '/home/chaddy/Keras_Testing/keras-tutorial/images/panda.jpg'\n",
    "\n",
    "catimage = cv.imread(dogPath)\n",
    "cv.imshow('dog', catimage)\n",
    "catOutput = catimage.copy()\n",
    "catimage = cv.resize(catimage, (pixel_new, pixel_new))\n",
    "\n",
    "catimage = image.astype(\"float\")/255.0\n",
    "\n",
    "catimage = catimage.flatten()\n",
    "catimage = catimage.reshape((1,catimage.shape[0]))\n",
    "\n",
    "print(catimage.shape)\n",
    "\n",
    "prediction = model.predict(catimage)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
